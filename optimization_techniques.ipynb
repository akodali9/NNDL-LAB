{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w,b,x): #sigmoid function\n",
    "    return 1.0/(1.0 +np.exp(-(w*x+b)))\n",
    "\n",
    "def error(w,b):\n",
    "    err = 0.0\n",
    "    for x,y in zip(X,Y):\n",
    "        fx = f(w,b,x)\n",
    "        err += 0.5 * (fx - y)**2\n",
    "    return err\n",
    "\n",
    "def grad_b(w,b,x,y):\n",
    "    fx = f(w,b,x)\n",
    "    return (fx - y) *fx * (1 - fx)\n",
    "\n",
    "def grad_w(w,b,x,y):\n",
    "    fx = f(w,b,x)\n",
    "    return (fx - y) *fx * (1 - fx) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_gradient_descent():\n",
    "    w, b, eta = init_w, init_b, 1.0\n",
    "    params = []\n",
    "    for i in range(1, max_iter+1):\n",
    "        dw, db = 0, 0\n",
    "        for x,y in zip(X,Y):\n",
    "            dw += grad_w(w, b, x, y)\n",
    "            db += grad_b(w, b, x, y)\n",
    "        params.append((w, b, error(w,b)))\n",
    "        w = w - eta * dw\n",
    "        b = b - eta * db \n",
    "    p = params[-1]\n",
    "    print(f'w = {p[0]:f}, b = {p[1]:f}, e = {p[2]:f}')\n",
    "\n",
    "\n",
    "def do_stochastic_gradient_descent():\n",
    "    w, b, eta = init_w, init_b, 1.0\n",
    "    params = []\n",
    "    batch_size = 1\n",
    "    for i in range(1, max_iter+1):\n",
    "        indexes = np.random.randint(0, len(X), batch_size) # random sample\n",
    "        Xs = np.take(X, indexes)\n",
    "        Ys = np.take(Y, indexes)\n",
    "        dw, db = 0, 0\n",
    "        for x,y in zip(Xs,Ys):\n",
    "            dw += grad_w(w, b, x, y)\n",
    "            db += grad_b(w, b, x, y)\n",
    "            params.append((w, b, error(w,b)))\n",
    "            w = w - eta * dw\n",
    "            b = b - eta * db\n",
    "    p = params[-1]\n",
    "    print(f'w = {p[0]:f}, b = {p[1]:f}, e = {p[2]:f}')\n",
    "\n",
    "    \n",
    "def do_mini_batch_gradient_descent():\n",
    "    w, b, eta = init_w, init_b, 1.0\n",
    "    params = []\n",
    "    batch_size = 2\n",
    "    for i in range(1, max_iter+1):\n",
    "        indexes = np.random.randint(0, len(X), batch_size) # random sample\n",
    "        Xs = np.take(X, indexes)\n",
    "        Ys = np.take(Y, indexes)\n",
    "        dw, db = 0, 0\n",
    "        for x,y in zip(Xs,Ys):\n",
    "            dw += grad_w(w, b, x, y)\n",
    "            db += grad_b(w, b, x, y)\n",
    "            params.append((w, b, error(w,b)))\n",
    "            w = w - eta * dw\n",
    "            b = b - eta * db\n",
    "    p = params[-1]\n",
    "    print(f'w = {p[0]:f}, b = {p[1]:f}, e = {p[2]:f}')\n",
    "\n",
    "    \n",
    "def do_momentum_gradient_descent():\n",
    "    w, b, eta = init_w, init_b, 1.0\n",
    "    v_w, v_b, prev_v_w, prev_v_b, gamma = 0, 0, 0, 0, 0.8\n",
    "    params = []\n",
    "    for i in range(1, max_iter+1):\n",
    "        dw, db = 0, 0\n",
    "        for x,y in zip(X,Y):\n",
    "            dw += grad_w(w, b, x, y)\n",
    "            db += grad_b(w, b, x, y)    \n",
    "        v_w = gamma * prev_v_w + eta*dw\n",
    "        v_b = gamma * prev_v_b + eta*db\n",
    "        w = w - gamma * prev_v_w + eta*dw\n",
    "        b = b - gamma * prev_v_b + eta*db\n",
    "        params.append((w, b, error(w,b)))\n",
    "        prev_v_w = v_w\n",
    "        prev_v_b = v_b\n",
    "    p = params[-1]\n",
    "    print(f'w = {p[0]:f}, b = {p[1]:f}, e = {p[2]:f}')\n",
    "\n",
    "    \n",
    "def do_nesterov_accelerated_gradient_descent():\n",
    "    w, b, eta = init_w, init_b, 1.0\n",
    "    prev_v_w, prev_v_b, gamma = 0, 0, 0.8\n",
    "    params = []\n",
    "    for i in range(1, max_iter+1):\n",
    "        dw, db = 0, 0\n",
    "        # do partial update\n",
    "        v_w = gamma * prev_v_w\n",
    "        v_b = gamma * prev_v_b \n",
    "        for x,y in zip(X,Y):\n",
    "          # calculate gradients after partial update\n",
    "            dw += grad_w(w - v_w, b - v_b, x, y)\n",
    "            db += grad_b(w - v_w, b - v_b, x, y)\n",
    "        # now do the full update    \n",
    "        v_w = gamma * prev_v_w + eta*dw\n",
    "        v_b = gamma * prev_v_b + eta*db\n",
    "        w = w - v_w\n",
    "        b = b - v_b\n",
    "        prev_v_w = v_w\n",
    "        prev_v_b = v_b\n",
    "        er = error(w, b)\n",
    "        params.append((w, b, er))\n",
    "    p = params[-1]\n",
    "    print(f'w = {p[0]:f}, b = {p[1]:f}, e = {p[2]:f}')\n",
    "\n",
    "    \n",
    "def do_adagrad():\n",
    "    w, b, eta = init_w, init_b, 0.1\n",
    "    v_w, v_b, eps = 0, 0, 1e-8\n",
    "    params = []\n",
    "    for i in range(1, max_iter+1):\n",
    "        dw, db = 0, 0\n",
    "        for x,y in zip(X,Y):\n",
    "            dw += grad_w(w, b, x, y)\n",
    "            db += grad_b(w, b, x, y)\n",
    "            \n",
    "        v_w = v_w + dw**2\n",
    "        v_b = v_b + db**2\n",
    "        w = w - (eta/np.sqrt(v_w + eps)) * dw\n",
    "        b = b - (eta/np.sqrt(v_b + eps)) * db\n",
    "        er = error(w, b)\n",
    "        params.append((w, b, er))\n",
    "    p = params[-1]\n",
    "    print(f'w = {p[0]:f}, b = {p[1]:f}, e = {p[2]:f}')\n",
    "  \n",
    "        \n",
    "def do_rmsprop():\n",
    "    w, b, eta = init_w, init_b, 0.1\n",
    "    v_w, v_b, beta, eps = 0, 0, 0.9, 1e-8\n",
    "    params = []\n",
    "    for i in range(1, max_iter+1):\n",
    "        dw, db = 0, 0\n",
    "        for x,y in zip(X,Y):\n",
    "            dw += grad_w(w, b, x, y)\n",
    "            db += grad_b(w, b, x, y)\n",
    "\n",
    "        v_w = beta * v_w + (1 - beta) * dw**2\n",
    "        v_b = beta * v_b + (1 - beta) * db**2\n",
    "\n",
    "        w = w - (eta/np.sqrt(v_w + eps)) * dw\n",
    "        b = b - (eta/np.sqrt(v_b + eps)) * db\n",
    "        er = error(w, b)\n",
    "        params.append((w, b, er))\n",
    "    p = params[-1]\n",
    "    print(f'w = {p[0]:f}, b = {p[1]:f}, e = {p[2]:f}')\n",
    "\n",
    "    \n",
    "def do_adam():\n",
    "    w, b, eta, max_epochs = 1, 1, 0.01, 100, \n",
    "    m_w, m_b, v_w, v_b, eps, beta1, beta2 = 0, 0, 0, 0, 1e-8, 0.9, 0.99\n",
    "    params = []\n",
    "    for i in range(1, max_iter+1):\n",
    "        dw, db = 0, 0\n",
    "        for x,y in zip(X,Y):\n",
    "            dw += grad_w(w, b, x, y)\n",
    "            db += grad_b(w, b, x, y)\n",
    "            m_w = beta1 * m_w + (1-beta1) * dw\n",
    "            m_b = beta1 * m_b + (1-beta1) * db\n",
    "            v_w = beta2 * v_w + (1-beta2) * dw**2\n",
    "            v_b = beta2 * v_b + (1-beta2) * db**2\n",
    "            m_w = m_w/(1-beta1**(i+1))\n",
    "            m_b = m_b/(1-beta1**(i+1))\n",
    "            v_w = v_w/(1-beta2**(i+1))\n",
    "            v_b = v_b/(1-beta2**(i+1))\n",
    "            w = w - eta * m_w/np.sqrt(v_w + eps)\n",
    "            b = b - eta * m_b/np.sqrt(v_b + eps)\n",
    "            er = error(w, b)\n",
    "            params.append((w, b, er))\n",
    "    p = params[-1]\n",
    "    print(f'w = {p[0]:f}, b = {p[1]:f}, e = {p[2]:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([2.5, 1.8])\n",
    "# Y = np.array([0.9, 0.5])\n",
    "\n",
    "eta = 1\n",
    "init_w = -2\n",
    "init_b = 2\n",
    "max_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_file = np.genfromtxt('mpg.csv', delimiter=',', skip_header=1)\n",
    "N = np.shape(X_file)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = X_file[:,4], X_file[:,0]\n",
    "\n",
    "# it is a good idea to normalize the data\n",
    "\n",
    "X = (X-np.mean(X))/np.std(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = -0.117372, b = 10.150044, e = 110658.404263\n"
     ]
    }
   ],
   "source": [
    "do_stochastic_gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = -0.242943, b = 11.137828, e = 110658.192456\n"
     ]
    }
   ],
   "source": [
    "do_mini_batch_gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 1740.552939, b = 3576.027209, e = 110658.070000\n"
     ]
    }
   ],
   "source": [
    "do_nesterov_accelerated_gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 346.510588, b = 716.805442, e = 110658.070000\n"
     ]
    }
   ],
   "source": [
    "do_gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = -0.329716, b = 5.188786, e = 110704.634872\n"
     ]
    }
   ],
   "source": [
    "do_adagrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\AppData\\Local\\Temp\\ipykernel_8640\\3021398965.py:156: RuntimeWarning: overflow encountered in scalar divide\n",
      "  v_w = v_w/(1-beta2**(i+1))\n",
      "C:\\Users\\akash\\AppData\\Local\\Temp\\ipykernel_8640\\3021398965.py:157: RuntimeWarning: overflow encountered in scalar divide\n",
      "  v_b = v_b/(1-beta2**(i+1))\n",
      "C:\\Users\\akash\\AppData\\Local\\Temp\\ipykernel_8640\\3021398965.py:154: RuntimeWarning: overflow encountered in scalar divide\n",
      "  m_w = m_w/(1-beta1**(i+1))\n",
      "C:\\Users\\akash\\AppData\\Local\\Temp\\ipykernel_8640\\3021398965.py:155: RuntimeWarning: overflow encountered in scalar divide\n",
      "  m_b = m_b/(1-beta1**(i+1))\n",
      "C:\\Users\\akash\\AppData\\Local\\Temp\\ipykernel_8640\\3021398965.py:158: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  w = w - eta * m_w/np.sqrt(v_w + eps)\n",
      "C:\\Users\\akash\\AppData\\Local\\Temp\\ipykernel_8640\\3021398965.py:159: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  b = b - eta * m_b/np.sqrt(v_b + eps)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive\\SEM_7\\NNDL\\NNDL LAB\\optimization_techniques.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/OneDrive/SEM_7/NNDL/NNDL%20LAB/optimization_techniques.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m do_adam()\n",
      "\u001b[1;32md:\\OneDrive\\SEM_7\\NNDL\\NNDL LAB\\optimization_techniques.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/OneDrive/SEM_7/NNDL/NNDL%20LAB/optimization_techniques.ipynb#X12sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m         w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39m eta \u001b[39m*\u001b[39m m_w\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39msqrt(v_w \u001b[39m+\u001b[39m eps)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/OneDrive/SEM_7/NNDL/NNDL%20LAB/optimization_techniques.ipynb#X12sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m         b \u001b[39m=\u001b[39m b \u001b[39m-\u001b[39m eta \u001b[39m*\u001b[39m m_b\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39msqrt(v_b \u001b[39m+\u001b[39m eps)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/OneDrive/SEM_7/NNDL/NNDL%20LAB/optimization_techniques.ipynb#X12sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m         er \u001b[39m=\u001b[39m error(w, b)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/OneDrive/SEM_7/NNDL/NNDL%20LAB/optimization_techniques.ipynb#X12sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m         params\u001b[39m.\u001b[39mappend((w, b, er))\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/OneDrive/SEM_7/NNDL/NNDL%20LAB/optimization_techniques.ipynb#X12sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m p \u001b[39m=\u001b[39m params[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32md:\\OneDrive\\SEM_7\\NNDL\\NNDL LAB\\optimization_techniques.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive/SEM_7/NNDL/NNDL%20LAB/optimization_techniques.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(X,Y):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive/SEM_7/NNDL/NNDL%20LAB/optimization_techniques.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     fx \u001b[39m=\u001b[39m f(w,b,x)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/OneDrive/SEM_7/NNDL/NNDL%20LAB/optimization_techniques.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m (fx \u001b[39m-\u001b[39m y)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive/SEM_7/NNDL/NNDL%20LAB/optimization_techniques.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m err\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "do_adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\AppData\\Local\\Temp\\ipykernel_8640\\2286617846.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0/(1.0 +np.exp(-(w*x+b)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 1043.531763, b = 2146.416325, e = 110658.070000\n"
     ]
    }
   ],
   "source": [
    "do_momentum_gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = -0.314940, b = 7.712244, e = 110661.823060\n"
     ]
    }
   ],
   "source": [
    "do_rmsprop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
